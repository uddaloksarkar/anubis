# **Anubis**

**Anubis** is a plagiarism detection tool designed for the following classroom scenario: given a set of submissions for an assignment, we want to decide whether a significant fraction of the cohort has plagiarised from an LLM or a minor fraction has. 

## Getting Started

1. Install basic requirements

```bash
conda create -n anubis python=3.10 -y
conda activate anubis
pip install --upgrade pip
pip install -r requirements.txt
```

2. Add `Human-Eval` repository as submodule

```bash
$ git submodule add org-14957082@github.com:openai/human-eval.git human_eval 
```

3. Download the Models

- [stable-code](https://huggingface.co/stabilityai/stable-code-3b)
- [deepseek-coder](https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base)
- [codegemma](https://huggingface.co/google/codegemma-2b)


# Components

The entire repository has three different components.

## 1. Sample Generation Component

**Anubis** requires samples from LLM to operate. The sample generation component consists of two steps: (1) generating samples from the LLM, and (2) filtering the correct samples.

### Generating Samples 
```bash
$ python get-samples.py  --modelID 0 --samples 2000 --taskID 0 --batch_size 10 --ndim 100 --seed 0
```
Parameters
- `modelID` - The LLM we want to generate sample from. Currently we support limited number of models (`stable-code`, `deepseek-coder`, `codegemma`) in `get-samples.py` but can easily be extended for new LLMs. To use `modelID=0` for `stable-code`, `modelID=1` for `deepseek-coder`, `modelID=2` for `codegemma`.
- `taskID` - Prompt Identifier from the Prompt Source. (Default prompt souce is `HumanEval.jsonl`).
- `samples` (Default: 2000) - Number of samples we want to generate for the test.
- `batch_size` (Default: 10) - Batch size for generation from LLM (Reduce batch size to overcome possible OOM error).
- `ndim` (Default: 100) - Maximum generation length of samples.
- `seed` (Default: 0) - Random seed.

Extra Parameters
- `temperature` (Default: 0.8) - Softmax temperature.
- `topp` (Default: 0.95) - $p$ value associated with Top-$p$ Sampling. 

### Filtering
```bash
$ python semantic-check.py  --inputdir ISAMPLEDIR --outputdir OSAMPLEDIR --taskID 0
```
Parameters
- `inputdir` - Input directory containing the generated samples.
- `outputdir` - Location output directory for the filtered samples. 
- `taskID` - Prompt Identifier from the Prompt Source (Default prompt souce is `HumanEval.jsonl`).


## 2. EVAL Component

Given a set of text samples EVAL component finds the generation probabilities of the elements in the set.
Using the EVAL component, we can evaluate the probability that a text element is generated from a selected LLM.

```bash
$ python do-eval.py --evalmodelID 1 --taskID 0 --batch_size 1 --smpsrc SAMPLEDIR 
```
Parameters
- `smpsrc` - Directory containing text samples to run EVAL on.
- `evalmodelID` - The *target* LLM from where we hypothesize the samples are generated from. Similar to `get-samples.py`, use `evalmodelID=0` for `stable-code`, `evalmodelID=1` for `deepseek-coder`, `evalmodelID=2` for `codegemma`.
- `taskID` - Prompt Identifier from the Prompt Source (Default prompt souce is `HumanEval.jsonl`).
- `batch_size` (Default: 1) - Batch size for generation from LLM (Reduce batch size to overcome possible OOM error).

Extra Parameters
- `temperature` (Default: 0.8) - The softmax temperature should match the value used during the generation process, if known. If the generation temperature is unknown, a default setting of 1 is recommended.
- `topp` (Default: 0.95) - $p$ value associated with Top-$p$ Sampling. `topp` should match the value used during the generation process, if known. If the generation Top-$p$ is unknown, a default setting of 1 is recommended.


## 3. Decision component

The decision component ultimately determines whether the submission set from the cohort is plagiarized.

```bash
$ python decider.py --evalmodelID 1 --studir EVALDIR1 --llmdir EVALDIR2 --lbsize 0.05 --bwidth 2 --sampthresh 500 --gthresh 0.08 --lthresh 100
```
Parameters
- `evalmodelID` - The *target* LLM from where we hypothesize the samples are generated from. Use `evalmodelID=0` for `stable-code`, `evalmodelID=1` for `deepseek-coder`, `evalmodelID=2` for `codegemma` 
- `studir` - Directory containing submissions from students
- `llmdir` - Directory containing samples generated from the *target* LLM
- `lbsize` (Default: 0.05) - fraction of samples to fall into minimum probability bucket
- `sampthresh` (Default: 500) - Minimum number of samples in the directories to run `decider.py`
- `bwidth` (Default: 2) - Bucket Width: The ratio between the maximum and minimum probabilities within a bucket.
- `gthresh` (Default: 0.08) - Threshold for global test
- `lthresh` (Default: 100) - Threshold for local test

#### Example Run

We have prepared a set of files generated by the EVAL component for a quick hands-on experience with the decision component on CPU machines.

1. The folder `eval_plagged_samples` contains evaluated probabilities provided by the EVAL component for a set of plagiarized submissions.
2. The folder `eval_nonplagged_samples` contains evaluated probabilities provided by the EVAL component for a set of non-plagiarized, authentic submissions.
3. The folder `eval_llm_samples` contains evaluated probabilities provided by the EVAL component for a set of samples generated by the LLM.

To run the decider on plagiarized samples for `task_id` 1 from the `Human-Eval` dataset: 
```bash
$ python decider.py --evalmodelID 1 --studir eval_plagged_samples/task01 --llmdir eval_llm_samples/task01 --lbsize 0.05 --bwidth 2 --sampthresh 500 --gthresh 0.08 --lthresh 100

FLAGGED
```

To run the decider on plagiarized samples for `task_id` 1 from the `Human-Eval` dataset: 

```bash
$ python decider.py --evalmodelID 1 --studir eval_nonplagged_samples/task01 --llmdir eval_llm_samples/task01 --lbsize 0.05 --bwidth 2 --sampthresh 500 --gthresh 0.08 --lthresh 100

PASSED
```